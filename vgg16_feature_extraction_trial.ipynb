{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg16_feature_extraction_trial.ipynb",
      "provenance": [],
      "mount_file_id": "1hiZll0AIkCVMkpJP9MIGuP_uSllwueHi",
      "authorship_tag": "ABX9TyN+4hyboPXgG8nuplORkmo0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akumarpandey686/DataPreprocessing_Code/blob/master/vgg16_feature_extraction_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZtxGtoYvCxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.applications import VGG16"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-IkVlCavnHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-size all the images to this\n",
        "img_width, img_height = 224, 224  # Default input size for VGG16\n",
        "\n",
        "train_dir = '/content/drive/My Drive/Data/train'\n",
        "valid_dir = '/content/drive/My Drive/Data/test'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DsCzAN4vtFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "a2fbde6a-00b9-45ee-b949-2bbeb7c6323c"
      },
      "source": [
        "# Instantiate convolutional base\n",
        "\n",
        "conv_base = VGG16(weights='imagenet', \n",
        "                  include_top=False,\n",
        "                  input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Show architecture\n",
        "conv_base.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56bByIYdv3lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract features\n",
        "import os, shutil\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggPrS69kzu7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bcd50e31-21cc-4dbf-cbde-75e1d7b80d4e"
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 5\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 7, 7, 512))  # Must be equal to the output of the convolutional base\n",
        "    labels = np.zeros(shape=(sample_count,2))\n",
        "    # Preprocess data\n",
        "    generator = datagen.flow_from_directory(directory,\n",
        "                                            target_size=(img_width,img_height),\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode='categorical')\n",
        "    # Pass data through convolutional base\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)\n",
        "        features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
        "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            break\n",
        "    return features, labels\n",
        "    \n",
        "train_features, train_labels = extract_features(train_dir, 173)  # Agree with our small dataset size\n",
        "validation_features, validation_labels = extract_features(valid_dir, 18)\n",
        "#test_features, test_labels = extract_features(test_dir, test_size)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 173 images belonging to 2 classes.\n",
            "Found 18 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwX-VD42zyDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63c987c3-a920-4ca9-9075-fb5d8197004a"
      },
      "source": [
        "train_labels\n",
        "print(len(train_labels))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsqh0gsdz1d9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9779da56-a38a-4482-b66f-645c98887a1f"
      },
      "source": [
        "epochs = 150\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GlobalAveragePooling2D(input_shape=(7,7,512)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "global_average_pooling2d_2 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 1,026\n",
            "Trainable params: 1,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jY9CELcz5SL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c704278d-6d8d-4962-8558-e42c171cb435"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint('model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  \n",
        "\n",
        "# Compile model\n",
        "from keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size, \n",
        "                    callbacks=[checkpoint],\n",
        "                    validation_data=(validation_features, validation_labels))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.7036 - acc: 0.5111\n",
            "Epoch 00001: val_loss improved from inf to 0.67882, saving model to model-001-0.618497-0.500000.h5\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.6226 - acc: 0.6185 - val_loss: 0.6788 - val_acc: 0.5000\n",
            "Epoch 2/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.5136 - acc: 0.7572\n",
            "Epoch 00002: val_loss improved from 0.67882 to 0.47718, saving model to model-002-0.757225-0.833333.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.5136 - acc: 0.7572 - val_loss: 0.4772 - val_acc: 0.8333\n",
            "Epoch 3/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.4389 - acc: 0.8497\n",
            "Epoch 00003: val_loss improved from 0.47718 to 0.42858, saving model to model-003-0.849711-0.833333.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.4389 - acc: 0.8497 - val_loss: 0.4286 - val_acc: 0.8333\n",
            "Epoch 4/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.4006 - acc: 0.8303\n",
            "Epoch 00004: val_loss improved from 0.42858 to 0.37101, saving model to model-004-0.838150-0.833333.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3922 - acc: 0.8382 - val_loss: 0.3710 - val_acc: 0.8333\n",
            "Epoch 5/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.8529\n",
            "Epoch 00005: val_loss improved from 0.37101 to 0.34469, saving model to model-005-0.855491-0.833333.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3533 - acc: 0.8555 - val_loss: 0.3447 - val_acc: 0.8333\n",
            "Epoch 6/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.3456 - acc: 0.8222\n",
            "Epoch 00006: val_loss improved from 0.34469 to 0.31737, saving model to model-006-0.855491-0.833333.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3240 - acc: 0.8555 - val_loss: 0.3174 - val_acc: 0.8333\n",
            "Epoch 7/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.2850 - acc: 0.8842\n",
            "Epoch 00007: val_loss improved from 0.31737 to 0.29649, saving model to model-007-0.855491-0.888889.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.3036 - acc: 0.8555 - val_loss: 0.2965 - val_acc: 0.8889\n",
            "Epoch 8/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.2758 - acc: 0.8737\n",
            "Epoch 00008: val_loss improved from 0.29649 to 0.28620, saving model to model-008-0.884393-0.833333.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2759 - acc: 0.8844 - val_loss: 0.2862 - val_acc: 0.8333\n",
            "Epoch 9/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.3050 - acc: 0.8421\n",
            "Epoch 00009: val_loss improved from 0.28620 to 0.26884, saving model to model-009-0.884393-0.888889.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2656 - acc: 0.8844 - val_loss: 0.2688 - val_acc: 0.8889\n",
            "Epoch 10/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.2435 - acc: 0.9480\n",
            "Epoch 00010: val_loss improved from 0.26884 to 0.25924, saving model to model-010-0.947977-0.888889.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2435 - acc: 0.9480 - val_loss: 0.2592 - val_acc: 0.8889\n",
            "Epoch 11/150\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.2299 - acc: 0.9000\n",
            "Epoch 00011: val_loss improved from 0.25924 to 0.24817, saving model to model-011-0.907514-0.888889.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2296 - acc: 0.9075 - val_loss: 0.2482 - val_acc: 0.8889\n",
            "Epoch 12/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.2362 - acc: 0.9444\n",
            "Epoch 00012: val_loss improved from 0.24817 to 0.23935, saving model to model-012-0.959538-0.888889.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2186 - acc: 0.9595 - val_loss: 0.2394 - val_acc: 0.8889\n",
            "Epoch 13/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.2014 - acc: 0.9684\n",
            "Epoch 00013: val_loss improved from 0.23935 to 0.22648, saving model to model-013-0.965318-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.2056 - acc: 0.9653 - val_loss: 0.2265 - val_acc: 0.9444\n",
            "Epoch 14/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1937 - acc: 0.9653\n",
            "Epoch 00014: val_loss improved from 0.22648 to 0.22116, saving model to model-014-0.965318-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1937 - acc: 0.9653 - val_loss: 0.2212 - val_acc: 0.9444\n",
            "Epoch 15/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1847 - acc: 0.9895\n",
            "Epoch 00015: val_loss improved from 0.22116 to 0.21417, saving model to model-015-0.971098-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1853 - acc: 0.9711 - val_loss: 0.2142 - val_acc: 0.9444\n",
            "Epoch 16/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1534 - acc: 0.9789\n",
            "Epoch 00016: val_loss improved from 0.21417 to 0.20577, saving model to model-016-0.965318-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1760 - acc: 0.9653 - val_loss: 0.2058 - val_acc: 0.9444\n",
            "Epoch 17/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1658 - acc: 0.9684\n",
            "Epoch 00017: val_loss improved from 0.20577 to 0.19931, saving model to model-017-0.971098-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1677 - acc: 0.9711 - val_loss: 0.1993 - val_acc: 0.9444\n",
            "Epoch 18/150\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.1666 - acc: 0.9688\n",
            "Epoch 00018: val_loss improved from 0.19931 to 0.19261, saving model to model-018-0.971098-0.944444.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1609 - acc: 0.9711 - val_loss: 0.1926 - val_acc: 0.9444\n",
            "Epoch 19/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1547 - acc: 0.9711\n",
            "Epoch 00019: val_loss improved from 0.19261 to 0.18571, saving model to model-019-0.971098-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1547 - acc: 0.9711 - val_loss: 0.1857 - val_acc: 0.9444\n",
            "Epoch 20/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.1372 - acc: 0.9667\n",
            "Epoch 00020: val_loss improved from 0.18571 to 0.18005, saving model to model-020-0.971098-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1486 - acc: 0.9711 - val_loss: 0.1800 - val_acc: 0.9444\n",
            "Epoch 21/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1378 - acc: 0.9789\n",
            "Epoch 00021: val_loss improved from 0.18005 to 0.17923, saving model to model-021-0.976879-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1438 - acc: 0.9769 - val_loss: 0.1792 - val_acc: 0.9444\n",
            "Epoch 22/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1357 - acc: 0.9769\n",
            "Epoch 00022: val_loss improved from 0.17923 to 0.17076, saving model to model-022-0.976879-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1357 - acc: 0.9769 - val_loss: 0.1708 - val_acc: 0.9444\n",
            "Epoch 23/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1314 - acc: 0.9769\n",
            "Epoch 00023: val_loss improved from 0.17076 to 0.16572, saving model to model-023-0.976879-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1314 - acc: 0.9769 - val_loss: 0.1657 - val_acc: 0.9444\n",
            "Epoch 24/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1260 - acc: 0.9827\n",
            "Epoch 00024: val_loss improved from 0.16572 to 0.16260, saving model to model-024-0.982659-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1260 - acc: 0.9827 - val_loss: 0.1626 - val_acc: 0.9444\n",
            "Epoch 25/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.1223 - acc: 0.9765\n",
            "Epoch 00025: val_loss improved from 0.16260 to 0.15782, saving model to model-025-0.976879-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1212 - acc: 0.9769 - val_loss: 0.1578 - val_acc: 0.9444\n",
            "Epoch 26/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1115 - acc: 0.9789\n",
            "Epoch 00026: val_loss improved from 0.15782 to 0.15267, saving model to model-026-0.988439-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1165 - acc: 0.9884 - val_loss: 0.1527 - val_acc: 0.9444\n",
            "Epoch 27/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.1136 - acc: 0.9818\n",
            "Epoch 00027: val_loss improved from 0.15267 to 0.14977, saving model to model-027-0.982659-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1130 - acc: 0.9827 - val_loss: 0.1498 - val_acc: 0.9444\n",
            "Epoch 28/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1165 - acc: 0.9789\n",
            "Epoch 00028: val_loss improved from 0.14977 to 0.14930, saving model to model-028-0.982659-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1111 - acc: 0.9827 - val_loss: 0.1493 - val_acc: 0.9444\n",
            "Epoch 29/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.1060 - acc: 0.9884\n",
            "Epoch 00029: val_loss improved from 0.14930 to 0.14140, saving model to model-029-0.988439-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1060 - acc: 0.9884 - val_loss: 0.1414 - val_acc: 0.9444\n",
            "Epoch 30/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1074 - acc: 0.9789\n",
            "Epoch 00030: val_loss improved from 0.14140 to 0.13857, saving model to model-030-0.988439-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1025 - acc: 0.9884 - val_loss: 0.1386 - val_acc: 0.9444\n",
            "Epoch 31/150\n",
            "29/35 [=======================>......] - ETA: 0s - loss: 0.1085 - acc: 0.9793\n",
            "Epoch 00031: val_loss improved from 0.13857 to 0.13458, saving model to model-031-0.982659-0.944444.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1006 - acc: 0.9827 - val_loss: 0.1346 - val_acc: 0.9444\n",
            "Epoch 32/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1087 - acc: 0.9895\n",
            "Epoch 00032: val_loss improved from 0.13458 to 0.13196, saving model to model-032-0.988439-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0957 - acc: 0.9884 - val_loss: 0.1320 - val_acc: 0.9444\n",
            "Epoch 33/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.1013 - acc: 0.9789\n",
            "Epoch 00033: val_loss improved from 0.13196 to 0.12858, saving model to model-033-0.988439-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0928 - acc: 0.9884 - val_loss: 0.1286 - val_acc: 0.9444\n",
            "Epoch 34/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0926 - acc: 0.9895\n",
            "Epoch 00034: val_loss improved from 0.12858 to 0.12440, saving model to model-034-0.988439-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0911 - acc: 0.9884 - val_loss: 0.1244 - val_acc: 0.9444\n",
            "Epoch 35/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0870 - acc: 0.9942\n",
            "Epoch 00035: val_loss improved from 0.12440 to 0.12311, saving model to model-035-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0870 - acc: 0.9942 - val_loss: 0.1231 - val_acc: 0.9444\n",
            "Epoch 36/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0849 - acc: 0.9942\n",
            "Epoch 00036: val_loss improved from 0.12311 to 0.12008, saving model to model-036-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0849 - acc: 0.9942 - val_loss: 0.1201 - val_acc: 0.9444\n",
            "Epoch 37/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9941\n",
            "Epoch 00037: val_loss improved from 0.12008 to 0.11667, saving model to model-037-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0830 - acc: 0.9942 - val_loss: 0.1167 - val_acc: 0.9444\n",
            "Epoch 38/150\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0842 - acc: 0.9935\n",
            "Epoch 00038: val_loss improved from 0.11667 to 0.11581, saving model to model-038-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0812 - acc: 0.9942 - val_loss: 0.1158 - val_acc: 0.9444\n",
            "Epoch 39/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0826 - acc: 0.9895\n",
            "Epoch 00039: val_loss improved from 0.11581 to 0.11238, saving model to model-039-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0790 - acc: 0.9942 - val_loss: 0.1124 - val_acc: 0.9444\n",
            "Epoch 40/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0694 - acc: 1.0000\n",
            "Epoch 00040: val_loss improved from 0.11238 to 0.10994, saving model to model-040-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0758 - acc: 0.9942 - val_loss: 0.1099 - val_acc: 0.9444\n",
            "Epoch 41/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0780 - acc: 0.9895\n",
            "Epoch 00041: val_loss improved from 0.10994 to 0.10923, saving model to model-041-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0750 - acc: 0.9942 - val_loss: 0.1092 - val_acc: 0.9444\n",
            "Epoch 42/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0719 - acc: 0.9942\n",
            "Epoch 00042: val_loss improved from 0.10923 to 0.10586, saving model to model-042-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0719 - acc: 0.9942 - val_loss: 0.1059 - val_acc: 0.9444\n",
            "Epoch 43/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0784 - acc: 0.9895\n",
            "Epoch 00043: val_loss improved from 0.10586 to 0.10328, saving model to model-043-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0709 - acc: 0.9942 - val_loss: 0.1033 - val_acc: 0.9444\n",
            "Epoch 44/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0687 - acc: 0.9942\n",
            "Epoch 00044: val_loss improved from 0.10328 to 0.10249, saving model to model-044-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0687 - acc: 0.9942 - val_loss: 0.1025 - val_acc: 0.9444\n",
            "Epoch 45/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0668 - acc: 0.9939\n",
            "Epoch 00045: val_loss improved from 0.10249 to 0.09897, saving model to model-045-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0667 - acc: 0.9942 - val_loss: 0.0990 - val_acc: 0.9444\n",
            "Epoch 46/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0655 - acc: 0.9942\n",
            "Epoch 00046: val_loss improved from 0.09897 to 0.09689, saving model to model-046-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0655 - acc: 0.9942 - val_loss: 0.0969 - val_acc: 0.9444\n",
            "Epoch 47/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0512 - acc: 1.0000\n",
            "Epoch 00047: val_loss improved from 0.09689 to 0.09516, saving model to model-047-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0657 - acc: 0.9942 - val_loss: 0.0952 - val_acc: 0.9444\n",
            "Epoch 48/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0626 - acc: 0.9942\n",
            "Epoch 00048: val_loss improved from 0.09516 to 0.09384, saving model to model-048-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0626 - acc: 0.9942 - val_loss: 0.0938 - val_acc: 0.9444\n",
            "Epoch 49/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0720 - acc: 0.9895\n",
            "Epoch 00049: val_loss improved from 0.09384 to 0.09172, saving model to model-049-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0609 - acc: 0.9942 - val_loss: 0.0917 - val_acc: 0.9444\n",
            "Epoch 50/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0593 - acc: 0.9942\n",
            "Epoch 00050: val_loss improved from 0.09172 to 0.09011, saving model to model-050-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0593 - acc: 0.9942 - val_loss: 0.0901 - val_acc: 0.9444\n",
            "Epoch 51/150\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0569 - acc: 0.9937\n",
            "Epoch 00051: val_loss improved from 0.09011 to 0.08872, saving model to model-051-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0583 - acc: 0.9942 - val_loss: 0.0887 - val_acc: 0.9444\n",
            "Epoch 52/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0571 - acc: 0.9942\n",
            "Epoch 00052: val_loss improved from 0.08872 to 0.08726, saving model to model-052-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0571 - acc: 0.9942 - val_loss: 0.0873 - val_acc: 0.9444\n",
            "Epoch 53/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0615 - acc: 0.9895\n",
            "Epoch 00053: val_loss improved from 0.08726 to 0.08550, saving model to model-053-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0553 - acc: 0.9942 - val_loss: 0.0855 - val_acc: 0.9444\n",
            "Epoch 54/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0431 - acc: 1.0000\n",
            "Epoch 00054: val_loss improved from 0.08550 to 0.08257, saving model to model-054-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0543 - acc: 0.9942 - val_loss: 0.0826 - val_acc: 0.9444\n",
            "Epoch 55/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0534 - acc: 0.9942\n",
            "Epoch 00055: val_loss improved from 0.08257 to 0.08235, saving model to model-055-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0534 - acc: 0.9942 - val_loss: 0.0823 - val_acc: 0.9444\n",
            "Epoch 56/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0642 - acc: 0.9895\n",
            "Epoch 00056: val_loss improved from 0.08235 to 0.08019, saving model to model-056-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0520 - acc: 0.9942 - val_loss: 0.0802 - val_acc: 0.9444\n",
            "Epoch 57/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0594 - acc: 0.9895\n",
            "Epoch 00057: val_loss improved from 0.08019 to 0.07903, saving model to model-057-0.994220-0.944444.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0511 - acc: 0.9942 - val_loss: 0.0790 - val_acc: 0.9444\n",
            "Epoch 58/150\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0520 - acc: 0.9935\n",
            "Epoch 00058: val_loss improved from 0.07903 to 0.07679, saving model to model-058-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0504 - acc: 0.9942 - val_loss: 0.0768 - val_acc: 1.0000\n",
            "Epoch 59/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0393 - acc: 1.0000\n",
            "Epoch 00059: val_loss improved from 0.07679 to 0.07614, saving model to model-059-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0486 - acc: 0.9942 - val_loss: 0.0761 - val_acc: 1.0000\n",
            "Epoch 60/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0420 - acc: 1.0000\n",
            "Epoch 00060: val_loss improved from 0.07614 to 0.07427, saving model to model-060-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0480 - acc: 0.9942 - val_loss: 0.0743 - val_acc: 1.0000\n",
            "Epoch 61/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0472 - acc: 0.9942\n",
            "Epoch 00061: val_loss did not improve from 0.07427\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0472 - acc: 0.9942 - val_loss: 0.0747 - val_acc: 0.9444\n",
            "Epoch 62/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0460 - acc: 0.9942\n",
            "Epoch 00062: val_loss improved from 0.07427 to 0.07176, saving model to model-062-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0460 - acc: 0.9942 - val_loss: 0.0718 - val_acc: 1.0000\n",
            "Epoch 63/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0476 - acc: 0.9942\n",
            "Epoch 00063: val_loss improved from 0.07176 to 0.07001, saving model to model-063-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0476 - acc: 0.9942 - val_loss: 0.0700 - val_acc: 1.0000\n",
            "Epoch 64/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0453 - acc: 0.9939\n",
            "Epoch 00064: val_loss did not improve from 0.07001\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0446 - acc: 0.9942 - val_loss: 0.0707 - val_acc: 1.0000\n",
            "Epoch 65/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9941\n",
            "Epoch 00065: val_loss improved from 0.07001 to 0.06988, saving model to model-065-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0441 - acc: 0.9942 - val_loss: 0.0699 - val_acc: 1.0000\n",
            "Epoch 66/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9941\n",
            "Epoch 00066: val_loss improved from 0.06988 to 0.06651, saving model to model-066-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0424 - acc: 0.9942 - val_loss: 0.0665 - val_acc: 1.0000\n",
            "Epoch 67/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0464 - acc: 0.9895\n",
            "Epoch 00067: val_loss improved from 0.06651 to 0.06642, saving model to model-067-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0415 - acc: 0.9942 - val_loss: 0.0664 - val_acc: 1.0000\n",
            "Epoch 68/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9941\n",
            "Epoch 00068: val_loss improved from 0.06642 to 0.06557, saving model to model-068-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0410 - acc: 0.9942 - val_loss: 0.0656 - val_acc: 1.0000\n",
            "Epoch 69/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0415 - acc: 0.9895\n",
            "Epoch 00069: val_loss improved from 0.06557 to 0.06491, saving model to model-069-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0402 - acc: 0.9942 - val_loss: 0.0649 - val_acc: 1.0000\n",
            "Epoch 70/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0402 - acc: 1.0000\n",
            "Epoch 00070: val_loss improved from 0.06491 to 0.06373, saving model to model-070-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0396 - acc: 0.9942 - val_loss: 0.0637 - val_acc: 1.0000\n",
            "Epoch 71/150\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0404 - acc: 0.9937\n",
            "Epoch 00071: val_loss improved from 0.06373 to 0.06172, saving model to model-071-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0395 - acc: 0.9942 - val_loss: 0.0617 - val_acc: 1.0000\n",
            "Epoch 72/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0455 - acc: 0.9895\n",
            "Epoch 00072: val_loss improved from 0.06172 to 0.06126, saving model to model-072-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0379 - acc: 0.9942 - val_loss: 0.0613 - val_acc: 1.0000\n",
            "Epoch 73/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0417 - acc: 0.9895\n",
            "Epoch 00073: val_loss did not improve from 0.06126\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0372 - acc: 0.9942 - val_loss: 0.0617 - val_acc: 1.0000\n",
            "Epoch 74/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0410 - acc: 0.9895\n",
            "Epoch 00074: val_loss improved from 0.06126 to 0.05937, saving model to model-074-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0369 - acc: 0.9942 - val_loss: 0.0594 - val_acc: 1.0000\n",
            "Epoch 75/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0313 - acc: 1.0000\n",
            "Epoch 00075: val_loss improved from 0.05937 to 0.05830, saving model to model-075-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0370 - acc: 0.9942 - val_loss: 0.0583 - val_acc: 1.0000\n",
            "Epoch 76/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0325 - acc: 1.0000\n",
            "Epoch 00076: val_loss improved from 0.05830 to 0.05808, saving model to model-076-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0363 - acc: 0.9942 - val_loss: 0.0581 - val_acc: 1.0000\n",
            "Epoch 77/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0367 - acc: 0.9939\n",
            "Epoch 00077: val_loss did not improve from 0.05808\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0366 - acc: 0.9942 - val_loss: 0.0584 - val_acc: 1.0000\n",
            "Epoch 78/150\n",
            "30/35 [========================>.....] - ETA: 0s - loss: 0.0347 - acc: 0.9933\n",
            "Epoch 00078: val_loss improved from 0.05808 to 0.05710, saving model to model-078-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0346 - acc: 0.9942 - val_loss: 0.0571 - val_acc: 1.0000\n",
            "Epoch 79/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0410 - acc: 0.9889\n",
            "Epoch 00079: val_loss improved from 0.05710 to 0.05544, saving model to model-079-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0338 - acc: 0.9942 - val_loss: 0.0554 - val_acc: 1.0000\n",
            "Epoch 80/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0216 - acc: 1.0000\n",
            "Epoch 00080: val_loss did not improve from 0.05544\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0328 - acc: 0.9942 - val_loss: 0.0555 - val_acc: 1.0000\n",
            "Epoch 81/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0388 - acc: 0.9895\n",
            "Epoch 00081: val_loss improved from 0.05544 to 0.05357, saving model to model-081-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0330 - acc: 0.9942 - val_loss: 0.0536 - val_acc: 1.0000\n",
            "Epoch 82/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0320 - acc: 0.9942\n",
            "Epoch 00082: val_loss did not improve from 0.05357\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0320 - acc: 0.9942 - val_loss: 0.0544 - val_acc: 1.0000\n",
            "Epoch 83/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0320 - acc: 0.9942\n",
            "Epoch 00083: val_loss improved from 0.05357 to 0.05198, saving model to model-083-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0320 - acc: 0.9942 - val_loss: 0.0520 - val_acc: 1.0000\n",
            "Epoch 84/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9941\n",
            "Epoch 00084: val_loss improved from 0.05198 to 0.05142, saving model to model-084-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0319 - acc: 0.9942 - val_loss: 0.0514 - val_acc: 1.0000\n",
            "Epoch 85/150\n",
            "29/35 [=======================>......] - ETA: 0s - loss: 0.0243 - acc: 1.0000\n",
            "Epoch 00085: val_loss did not improve from 0.05142\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0306 - acc: 0.9942 - val_loss: 0.0526 - val_acc: 1.0000\n",
            "Epoch 86/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0298 - acc: 0.9942\n",
            "Epoch 00086: val_loss improved from 0.05142 to 0.04932, saving model to model-086-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0298 - acc: 0.9942 - val_loss: 0.0493 - val_acc: 1.0000\n",
            "Epoch 87/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0341 - acc: 0.9895\n",
            "Epoch 00087: val_loss did not improve from 0.04932\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0291 - acc: 0.9942 - val_loss: 0.0495 - val_acc: 1.0000\n",
            "Epoch 88/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0333 - acc: 0.9895\n",
            "Epoch 00088: val_loss improved from 0.04932 to 0.04870, saving model to model-088-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0289 - acc: 0.9942 - val_loss: 0.0487 - val_acc: 1.0000\n",
            "Epoch 89/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0210 - acc: 1.0000\n",
            "Epoch 00089: val_loss improved from 0.04870 to 0.04850, saving model to model-089-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0284 - acc: 0.9942 - val_loss: 0.0485 - val_acc: 1.0000\n",
            "Epoch 90/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9942\n",
            "Epoch 00090: val_loss improved from 0.04850 to 0.04688, saving model to model-090-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0279 - acc: 0.9942 - val_loss: 0.0469 - val_acc: 1.0000\n",
            "Epoch 91/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0223 - acc: 1.0000\n",
            "Epoch 00091: val_loss did not improve from 0.04688\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0276 - acc: 0.9942 - val_loss: 0.0478 - val_acc: 1.0000\n",
            "Epoch 92/150\n",
            "28/35 [=======================>......] - ETA: 0s - loss: 0.0282 - acc: 0.9929\n",
            "Epoch 00092: val_loss improved from 0.04688 to 0.04600, saving model to model-092-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0274 - acc: 0.9942 - val_loss: 0.0460 - val_acc: 1.0000\n",
            "Epoch 93/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0264 - acc: 0.9942\n",
            "Epoch 00093: val_loss improved from 0.04600 to 0.04561, saving model to model-093-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0264 - acc: 0.9942 - val_loss: 0.0456 - val_acc: 1.0000\n",
            "Epoch 94/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0321 - acc: 0.9895\n",
            "Epoch 00094: val_loss improved from 0.04561 to 0.04538, saving model to model-094-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0262 - acc: 0.9942 - val_loss: 0.0454 - val_acc: 1.0000\n",
            "Epoch 95/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0321 - acc: 0.9895\n",
            "Epoch 00095: val_loss improved from 0.04538 to 0.04474, saving model to model-095-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0262 - acc: 0.9942 - val_loss: 0.0447 - val_acc: 1.0000\n",
            "Epoch 96/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0254 - acc: 0.9942\n",
            "Epoch 00096: val_loss improved from 0.04474 to 0.04452, saving model to model-096-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0254 - acc: 0.9942 - val_loss: 0.0445 - val_acc: 1.0000\n",
            "Epoch 97/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0271 - acc: 0.9895\n",
            "Epoch 00097: val_loss improved from 0.04452 to 0.04372, saving model to model-097-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0252 - acc: 0.9942 - val_loss: 0.0437 - val_acc: 1.0000\n",
            "Epoch 98/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0204 - acc: 1.0000\n",
            "Epoch 00098: val_loss did not improve from 0.04372\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0247 - acc: 0.9942 - val_loss: 0.0451 - val_acc: 1.0000\n",
            "Epoch 99/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0262 - acc: 0.9895\n",
            "Epoch 00099: val_loss improved from 0.04372 to 0.04213, saving model to model-099-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0242 - acc: 0.9942 - val_loss: 0.0421 - val_acc: 1.0000\n",
            "Epoch 100/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0238 - acc: 0.9942\n",
            "Epoch 00100: val_loss did not improve from 0.04213\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0238 - acc: 0.9942 - val_loss: 0.0425 - val_acc: 1.0000\n",
            "Epoch 101/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0236 - acc: 0.9942\n",
            "Epoch 00101: val_loss improved from 0.04213 to 0.04182, saving model to model-101-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0236 - acc: 0.9942 - val_loss: 0.0418 - val_acc: 1.0000\n",
            "Epoch 102/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9941\n",
            "Epoch 00102: val_loss improved from 0.04182 to 0.04085, saving model to model-102-0.994220-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0233 - acc: 0.9942 - val_loss: 0.0408 - val_acc: 1.0000\n",
            "Epoch 103/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0209 - acc: 1.0000\n",
            "Epoch 00103: val_loss did not improve from 0.04085\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0229 - acc: 0.9942 - val_loss: 0.0422 - val_acc: 1.0000\n",
            "Epoch 104/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0292 - acc: 1.0000\n",
            "Epoch 00104: val_loss improved from 0.04085 to 0.03915, saving model to model-104-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.0391 - val_acc: 1.0000\n",
            "Epoch 105/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9941\n",
            "Epoch 00105: val_loss did not improve from 0.03915\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0225 - acc: 0.9942 - val_loss: 0.0398 - val_acc: 1.0000\n",
            "Epoch 106/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0237 - acc: 1.0000\n",
            "Epoch 00106: val_loss improved from 0.03915 to 0.03888, saving model to model-106-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.0389 - val_acc: 1.0000\n",
            "Epoch 107/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0217 - acc: 1.0000\n",
            "Epoch 00107: val_loss improved from 0.03888 to 0.03871, saving model to model-107-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0387 - val_acc: 1.0000\n",
            "Epoch 108/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0210 - acc: 1.0000\n",
            "Epoch 00108: val_loss improved from 0.03871 to 0.03810, saving model to model-108-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0381 - val_acc: 1.0000\n",
            "Epoch 109/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0161 - acc: 1.0000\n",
            "Epoch 00109: val_loss did not improve from 0.03810\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0209 - acc: 0.9942 - val_loss: 0.0383 - val_acc: 1.0000\n",
            "Epoch 110/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0170 - acc: 1.0000\n",
            "Epoch 00110: val_loss improved from 0.03810 to 0.03797, saving model to model-110-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 1.0000\n",
            "Epoch 111/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0203 - acc: 1.0000\n",
            "Epoch 00111: val_loss improved from 0.03797 to 0.03681, saving model to model-111-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 1.0000\n",
            "Epoch 112/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0205 - acc: 0.9942\n",
            "Epoch 00112: val_loss did not improve from 0.03681\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0205 - acc: 0.9942 - val_loss: 0.0379 - val_acc: 1.0000\n",
            "Epoch 113/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 1.0000\n",
            "Epoch 00113: val_loss improved from 0.03681 to 0.03640, saving model to model-113-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0364 - val_acc: 1.0000\n",
            "Epoch 114/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0152 - acc: 1.0000\n",
            "Epoch 00114: val_loss did not improve from 0.03640\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.0369 - val_acc: 1.0000\n",
            "Epoch 115/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0188 - acc: 1.0000\n",
            "Epoch 00115: val_loss improved from 0.03640 to 0.03489, saving model to model-115-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0349 - val_acc: 1.0000\n",
            "Epoch 116/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0187 - acc: 1.0000\n",
            "Epoch 00116: val_loss did not improve from 0.03489\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0354 - val_acc: 1.0000\n",
            "Epoch 117/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 1.0000\n",
            "Epoch 00117: val_loss did not improve from 0.03489\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0351 - val_acc: 1.0000\n",
            "Epoch 118/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0189 - acc: 1.0000\n",
            "Epoch 00118: val_loss improved from 0.03489 to 0.03389, saving model to model-118-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0339 - val_acc: 1.0000\n",
            "Epoch 119/150\n",
            "30/35 [========================>.....] - ETA: 0s - loss: 0.0189 - acc: 1.0000\n",
            "Epoch 00119: val_loss did not improve from 0.03389\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0339 - val_acc: 1.0000\n",
            "Epoch 120/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 1.0000\n",
            "Epoch 00120: val_loss improved from 0.03389 to 0.03371, saving model to model-120-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0337 - val_acc: 1.0000\n",
            "Epoch 121/150\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0176 - acc: 1.0000\n",
            "Epoch 00121: val_loss improved from 0.03371 to 0.03352, saving model to model-121-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0335 - val_acc: 1.0000\n",
            "Epoch 122/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 1.0000\n",
            "Epoch 00122: val_loss improved from 0.03352 to 0.03274, saving model to model-122-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0327 - val_acc: 1.0000\n",
            "Epoch 123/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0188 - acc: 1.0000\n",
            "Epoch 00123: val_loss did not improve from 0.03274\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0328 - val_acc: 1.0000\n",
            "Epoch 124/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0170 - acc: 1.0000\n",
            "Epoch 00124: val_loss improved from 0.03274 to 0.03220, saving model to model-124-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0322 - val_acc: 1.0000\n",
            "Epoch 125/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0189 - acc: 1.0000\n",
            "Epoch 00125: val_loss did not improve from 0.03220\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0324 - val_acc: 1.0000\n",
            "Epoch 126/150\n",
            "30/35 [========================>.....] - ETA: 0s - loss: 0.0167 - acc: 1.0000\n",
            "Epoch 00126: val_loss improved from 0.03220 to 0.03188, saving model to model-126-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0319 - val_acc: 1.0000\n",
            "Epoch 127/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0154 - acc: 1.0000\n",
            "Epoch 00127: val_loss did not improve from 0.03188\n",
            "35/35 [==============================] - 0s 3ms/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0323 - val_acc: 1.0000\n",
            "Epoch 128/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0157 - acc: 1.0000\n",
            "Epoch 00128: val_loss improved from 0.03188 to 0.03133, saving model to model-128-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0313 - val_acc: 1.0000\n",
            "Epoch 129/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 1.0000\n",
            "Epoch 00129: val_loss improved from 0.03133 to 0.03098, saving model to model-129-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 1.0000\n",
            "Epoch 130/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0156 - acc: 1.0000\n",
            "Epoch 00130: val_loss improved from 0.03098 to 0.03027, saving model to model-130-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0303 - val_acc: 1.0000\n",
            "Epoch 131/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0123 - acc: 1.0000\n",
            "Epoch 00131: val_loss did not improve from 0.03027\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0314 - val_acc: 1.0000\n",
            "Epoch 132/150\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.0176 - acc: 1.0000\n",
            "Epoch 00132: val_loss improved from 0.03027 to 0.02952, saving model to model-132-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0295 - val_acc: 1.0000\n",
            "Epoch 133/150\n",
            "32/35 [==========================>...] - ETA: 0s - loss: 0.0151 - acc: 1.0000\n",
            "Epoch 00133: val_loss did not improve from 0.02952\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0302 - val_acc: 1.0000\n",
            "Epoch 134/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0163 - acc: 1.0000\n",
            "Epoch 00134: val_loss improved from 0.02952 to 0.02949, saving model to model-134-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0295 - val_acc: 1.0000\n",
            "Epoch 135/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0142 - acc: 1.0000\n",
            "Epoch 00135: val_loss did not improve from 0.02949\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0300 - val_acc: 1.0000\n",
            "Epoch 136/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0146 - acc: 1.0000\n",
            "Epoch 00136: val_loss improved from 0.02949 to 0.02910, saving model to model-136-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0291 - val_acc: 1.0000\n",
            "Epoch 137/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 1.0000\n",
            "Epoch 00137: val_loss improved from 0.02910 to 0.02840, saving model to model-137-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 1.0000\n",
            "Epoch 138/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0140 - acc: 1.0000\n",
            "Epoch 00138: val_loss did not improve from 0.02840\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0285 - val_acc: 1.0000\n",
            "Epoch 139/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 1.0000\n",
            "Epoch 00139: val_loss improved from 0.02840 to 0.02784, saving model to model-139-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 1.0000\n",
            "Epoch 140/150\n",
            "34/35 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 1.0000\n",
            "Epoch 00140: val_loss did not improve from 0.02784\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 1.0000\n",
            "Epoch 141/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0129 - acc: 1.0000\n",
            "Epoch 00141: val_loss improved from 0.02784 to 0.02741, saving model to model-141-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0274 - val_acc: 1.0000\n",
            "Epoch 142/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0131 - acc: 1.0000\n",
            "Epoch 00142: val_loss improved from 0.02741 to 0.02735, saving model to model-142-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 1.0000\n",
            "Epoch 143/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0126 - acc: 1.0000\n",
            "Epoch 00143: val_loss improved from 0.02735 to 0.02706, saving model to model-143-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0271 - val_acc: 1.0000\n",
            "Epoch 144/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0126 - acc: 1.0000\n",
            "Epoch 00144: val_loss improved from 0.02706 to 0.02671, saving model to model-144-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.0267 - val_acc: 1.0000\n",
            "Epoch 145/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0148 - acc: 1.0000\n",
            "Epoch 00145: val_loss improved from 0.02671 to 0.02663, saving model to model-145-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0266 - val_acc: 1.0000\n",
            "Epoch 146/150\n",
            "31/35 [=========================>....] - ETA: 0s - loss: 0.0126 - acc: 1.0000\n",
            "Epoch 00146: val_loss improved from 0.02663 to 0.02603, saving model to model-146-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0260 - val_acc: 1.0000\n",
            "Epoch 147/150\n",
            "19/35 [===============>..............] - ETA: 0s - loss: 0.0088 - acc: 1.0000    \n",
            "Epoch 00147: val_loss did not improve from 0.02603\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0260 - val_acc: 1.0000\n",
            "Epoch 148/150\n",
            "33/35 [===========================>..] - ETA: 0s - loss: 0.0121 - acc: 1.0000\n",
            "Epoch 00148: val_loss improved from 0.02603 to 0.02601, saving model to model-148-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.0260 - val_acc: 1.0000\n",
            "Epoch 149/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 00149: val_loss improved from 0.02601 to 0.02577, saving model to model-149-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0258 - val_acc: 1.0000\n",
            "Epoch 150/150\n",
            "35/35 [==============================] - ETA: 0s - loss: 0.0114 - acc: 1.0000\n",
            "Epoch 00150: val_loss improved from 0.02577 to 0.02500, saving model to model-150-1.000000-1.000000.h5\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0250 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doqiK7dY0Kj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "3d91958b-e54a-4cb4-fb81-fa051fe25a97"
      },
      "source": [
        "# Plot results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pytd1G5A0PuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "def prediction(img_path):\n",
        "    org_img = image.load_img(img_path)\n",
        "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
        "    img_tensor = image.img_to_array(img)  # Image data encoded as integers in the 0–255 range\n",
        "    img_tensor /= 255.  # Normalize to [0,1] for plt.imshow application\n",
        "    plt.imshow(org_img)                           \n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Extract features\n",
        "    features = conv_base.predict(img_tensor.reshape(1,img_width, img_height, 3))\n",
        "\n",
        "    # Make prediction\n",
        "    try:\n",
        "        prediction = model.predict(features)\n",
        "    except:\n",
        "        prediction = model.predict(features.reshape(1, 7*7*512))\n",
        "        \n",
        "    classes = [\"np\", \"p\"]\n",
        "    print(\"I see...\"+str(classes[np.argmax(np.array(prediction[0]))]))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0-04iP4vAOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#refer https://www.youtube.com/watch?v=PaSEVY9d4RI&t=1272s"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjiCeNRCve4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}